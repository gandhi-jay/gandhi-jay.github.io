<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Day 2 of Ranking documents (DTM, TF and TF-IDF) and Difference between Relevance and Similarity | Jay Gandhi</title><meta name=keywords content="ai,rag,graphrag,llm"><meta name=description content="AI Days - Day 1 of N, Start of exploring AI"><meta name=author content="Jay Gandhi"><link rel=canonical href=https://www.gandhijay.com/ai/002/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://www.gandhijay.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.gandhijay.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.gandhijay.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.gandhijay.com/apple-touch-icon.png><link rel=mask-icon href=https://www.gandhijay.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://www.gandhijay.com/ai/002/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-R83NFY43QT"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R83NFY43QT")}</script><meta property="og:url" content="https://www.gandhijay.com/ai/002/"><meta property="og:site_name" content="Jay Gandhi"><meta property="og:title" content="Day 2 of Ranking documents (DTM, TF and TF-IDF) and Difference between Relevance and Similarity"><meta property="og:description" content="AI Days - Day 1 of N, Start of exploring AI"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2025-09-16T22:54:21+05:30"><meta property="article:modified_time" content="2025-09-16T22:54:21+05:30"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Rag"><meta property="article:tag" content="Graphrag"><meta property="article:tag" content="Llm"><meta name=twitter:card content="summary"><meta name=twitter:title content="Day 2 of Ranking documents (DTM, TF and TF-IDF) and Difference between Relevance and Similarity"><meta name=twitter:description content="AI Days - Day 1 of N, Start of exploring AI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AI Days","item":"https://www.gandhijay.com/ai/"},{"@type":"ListItem","position":2,"name":"Day 2 of Ranking documents (DTM, TF and TF-IDF) and Difference between Relevance and Similarity","item":"https://www.gandhijay.com/ai/002/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Day 2 of Ranking documents (DTM, TF and TF-IDF) and Difference between Relevance and Similarity","name":"Day 2 of Ranking documents (DTM, TF and TF-IDF) and Difference between Relevance and Similarity","description":"AI Days - Day 1 of N, Start of exploring AI","keywords":["ai","rag","graphrag","llm"],"articleBody":"Day 2 This post is not so AI/ML related at the start but eventually I’ll dive deeper. These are the notes I’ve taken which might makes sense for Knowledge Graph or RAG with the big picture. At the very base its Document-term matrix [1] .\nRanking documents, Document-term Matrix (DTM), TF and TF-IDF DTM is matrix that describes frequency of the terms (words) occur in each document so that can be analyzed with statistical or ML models. In below example, rows represents documents (sentences) and columns represents terms (unique words appreared in each sentences). The values are frequency or weight of the terms appeared in document.\nExample There are three documents\nLooking for the item in a store? On which aisle it is in a store? Looking for aisle 1A. 1a aisle for in is it item looking on store the which Doc1 0 0 1 1 0 0 1 1 0 1 1 0 Doc2 0 1 0 1 1 1 0 0 1 1 0 1 Doc3 1 1 1 0 0 0 0 1 0 0 0 0 Checkout python code for this at: DTM-TF-Only-Counts.ipynb normalized TF and TF-IDF This is one variation and another variation is to normalize which is to divide each term frequency by total words in a document.\n$$ TD(f,d) = \\frac{\\text{Number of times term t appears in document d}}{\\text{Total number of terms in document d}} $$\nTF-IDF (Term Frequency-Inverse Document Frequency) weights terms by how important they are relative to the entire collection of document.\n$$ TF{\\text -}IDF(f,d) = {TF(f,d)}\\times{IDF(t)} $$\nwhere $IDF$ is,\n$$ IDF = log{\\frac{\\text{Total number of documents}}{ 1 + {\\text{total number of documents containing term t}}}} $$\nIn our corpus (collection), frequent words will get more weightage where $TF{\\text -}IDF$ indicates how rare the word is across. $IDF$ shows terms that appear in many documents (even stopwords) get downweighted and rare words (might be relevant words) get upweighted. This removes bias of common terms which dominated the results in TF. Intituion is if High TF and its rare word (low IDF) then it will get higher TF-IDF. Check out the gist above for TF-IDF which uses TfidfVectorizer .\nJust found out about Okapi family of ranking function and one of the ranking function BM25 (Best match 25) [2] which are widely used now than TF-IDF.\nThese are commonly used in Information Retrieval (IR), including applications such as search engines, document clustering and classification, keyword extraction, and text similarity measures.\nRelevance \u0026 Similarity So what’s difference between relevance and similarity? Both sounds same but they are not in IR or NLP. This difference will come in handy in future as well.\nRelevance Relevance refers to how helpful or suitable a document is for a given query or task. For example, if you search for “My car broke down, what to do?” and there’s an article titled “Story: My car broke down, what I did,” the words may match closely, but that article isn’t really useful when you’re stranded on the road looking for guidance.\nMost relevant result would be “Automobile roadside assistance guide”, no overlap with word but highly relevant. Relevance incorporates user-intent, context and importance of the terms. Relevance is context-dependent and asymmetric,\n$$ relevance(Query, Doc) \\neq relevance(Doc, Query) $$\nSimilarity Similarity is numrical measure of how terms are alike based on some metric and it is usally symmatric,\n$$ similarity(A, B) == similarity(B, A) $$\nSimilarity measures common features, words, structure or patterns and it is context-independent usually. It answers how these things look alike. A black shoe is highly similar with another black shoe of the same company.\nOverlap These concepts do overlap as,\nsimilarity is often a strong signal for relevance. In many cases, most relevant item is also the most similar one.\nAs above, this relationship breaks down due to Synonymy \u0026 Language Variation (most relavant result may not be the most similar) and Polysemy \u0026 Ambiguity (High similar results can be irrelevant)\nWhat did we calculate above? Are TF-IDF and BM25 measures of relevance or similarity? Ranking functions primarily measure the similarity between a query and a document. This similarity serves as the basis for estimating relevance in search. In other words, ranking functions are similarity-based mechanisms designed to achieve relevance ranking.\nCosine Similarity This metrics is used to measure how similar two vector are, irrespective of their length. There’s awesome video on Youtube by Josh Starmer Cosine Similarity, Clearly Explained!!! . Do watch!\nCore idea is based on how to consider importance of the terms and overall direction of the content, not just the frequency of terms. It measures the cosine of the angle between two vectors.\nSmall angle, both document point in same direction, High cosine similarity (close to 1), Very Similar $90^\\circ$ angle $cos(90^\\circ) = 0$, No similarity Obtuse angle, they point in opposite directions, negative cosine similarity, opposites Code snippet of measuring is Cosine Similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import cosine_similarity import pandas as pd # Sample documents docs = [ \"Looking for the item in a store?\", \"On which aisle it is in a store?\", \"Looking for aisle 1A.\" ] # ---------- Step 1: TF-IDF ---------- vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(docs) # Convert to DataFrame for readability tfidf_df = pd.DataFrame( tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=[f\"Doc{i+1}\" for i in range(len(docs))] ) print(\"=== TF-IDF Matrix ===\") print(tfidf_df, \"\\n\") # ---------- Step 2: Cosine Similarity ---------- cos_sim = cosine_similarity(tfidf_matrix) cos_sim_df = pd.DataFrame( cos_sim, index=[f\"Doc{i+1}\" for i in range(len(docs))], columns=[f\"Doc{i+1}\" for i in range(len(docs))] ) print(cos_sim_df) Bibliography [1] Document-term matrix - https://en.wikipedia.org/wiki/Document-term_matrix [2] Okapi - BM25 - https://en.wikipedia.org/wiki/Okapi_BM25 ","wordCount":"927","inLanguage":"en","datePublished":"2025-09-16T22:54:21+05:30","dateModified":"2025-09-16T22:54:21+05:30","author":{"@type":"Person","name":"Jay Gandhi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.gandhijay.com/ai/002/"},"publisher":{"@type":"Organization","name":"Jay Gandhi","logo":{"@type":"ImageObject","url":"https://www.gandhijay.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.gandhijay.com/ accesskey=h title="Jay Gandhi (Alt + H)">Jay Gandhi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.gandhijay.com/about/ title=About><span>About</span></a></li><li><a href=https://www.gandhijay.com/ai/ title="AI Days"><span>AI Days</span></a></li><li><a href=https://www.gandhijay.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://www.gandhijay.com/bookshelf/ title=Bookshelf><span>Bookshelf</span></a></li><li><a href=https://www.gandhijay.com/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.gandhijay.com/>Home</a>&nbsp;»&nbsp;<a href=https://www.gandhijay.com/ai/>AI Days</a></div><h1 class="post-title entry-hint-parent">Day 2 of Ranking documents (DTM, TF and TF-IDF) and Difference between Relevance and Similarity</h1><div class=post-description>AI Days - Day 1 of N, Start of exploring AI</div><div class=post-meta><span title='2025-09-16 22:54:21 +0530 IST'>September 16, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Jay Gandhi</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#day-2>Day 2</a><ul><li><a href=#ranking-documents-document-term-matrix-dtm-tf-and-tf-idf>Ranking documents, Document-term Matrix (DTM), TF and TF-IDF</a></li><li><a href=#normalized-tf-and-tf-idf>normalized TF and TF-IDF</a></li></ul></li><li><a href=#relevance--similarity>Relevance & Similarity</a><ul><li><a href=#relevance>Relevance</a></li><li><a href=#similarity>Similarity</a></li><li><a href=#overlap>Overlap</a></li><li><a href=#what-did-we-calculate-above>What did we calculate above?</a></li></ul></li><li><a href=#cosine-similarity>Cosine Similarity</a></li><li><a href=#bibliography>Bibliography</a></li></ul></nav></div></details></div><div class=post-content><h2 id=day-2>Day 2<a hidden class=anchor aria-hidden=true href=#day-2>#</a></h2><p>This post is not so AI/ML related at the start but eventually I&rsquo;ll dive deeper. These are the notes I&rsquo;ve taken which might makes sense for Knowledge Graph or RAG with the big picture. At the very base its Document-term matrix <a href=#bibliography>[1]</a>
.</p><h3 id=ranking-documents-document-term-matrix-dtm-tf-and-tf-idf>Ranking documents, Document-term Matrix (DTM), TF and TF-IDF<a hidden class=anchor aria-hidden=true href=#ranking-documents-document-term-matrix-dtm-tf-and-tf-idf>#</a></h3><p>DTM is matrix that describes frequency of the terms (words) occur in each document so that can be analyzed with statistical or ML models. In below example, rows represents documents (sentences) and columns represents terms (unique words appreared in each sentences). The values are frequency or weight of the terms appeared in document.</p><h4 id=example>Example<a hidden class=anchor aria-hidden=true href=#example>#</a></h4><p>There are three documents</p><ol><li>Looking for the item in a store?</li><li>On which aisle it is in a store?</li><li>Looking for aisle 1A.</li></ol><table><thead><tr><th></th><th>1a</th><th>aisle</th><th>for</th><th>in</th><th>is</th><th>it</th><th>item</th><th>looking</th><th>on</th><th>store</th><th>the</th><th>which</th></tr></thead><tbody><tr><td>Doc1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>Doc2</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td></tr><tr><td>Doc3</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table><p>Checkout python code for this at: <a href=https://gist.github.com/gandhi-jay/2b7acc9d0e61513ffb1c80fbd80ea2cd target=_blank rel=noopener>DTM-TF-Only-Counts.ipynb</a></p><h3 id=normalized-tf-and-tf-idf>normalized TF and TF-IDF<a hidden class=anchor aria-hidden=true href=#normalized-tf-and-tf-idf>#</a></h3><p>This is one variation and another variation is to normalize which is to divide each term frequency by total words in a document.</p><p>$$
TD(f,d) = \frac{\text{Number of times term t appears in document d}}{\text{Total number of terms in document d}}
$$</p><p>TF-IDF (Term Frequency-Inverse Document Frequency) weights terms by how important they are relative to the entire collection of document.</p><p>$$
TF{\text -}IDF(f,d) = {TF(f,d)}\times{IDF(t)}
$$</p><p>where $IDF$ is,</p><p>$$
IDF = log{\frac{\text{Total number of documents}}{ 1 + {\text{total number of documents containing term t}}}}
$$</p><p>In our corpus (collection), frequent words will get more weightage where $TF{\text -}IDF$ indicates how rare the word is across. $IDF$ shows terms that appear in many documents (even stopwords) get downweighted and rare words (might be relevant words) get upweighted. This removes bias of common terms which dominated the results in TF. Intituion is if High TF and its rare word (low IDF) then it will get higher TF-IDF. Check out the gist above for TF-IDF which uses <a href=https://www.kaggle.com/code/zeeshanlatif/countvectorizer-vs-tfidfvectorizer target=_blank rel=noopener><code>TfidfVectorizer</code></a>
.</p><p>Just found out about Okapi family of ranking function and one of the ranking function BM25 (Best match 25) <a href=#bibliography>[2]</a>
which are widely used now than TF-IDF.</p><p>These are commonly used in Information Retrieval (IR), including applications such as search engines, document clustering and classification, keyword extraction, and text similarity measures.</p><h2 id=relevance--similarity>Relevance & Similarity<a hidden class=anchor aria-hidden=true href=#relevance--similarity>#</a></h2><p>So what&rsquo;s difference between relevance and similarity? Both sounds same but they are not in IR or NLP. This difference will come in handy in future as well.</p><h3 id=relevance>Relevance<a hidden class=anchor aria-hidden=true href=#relevance>#</a></h3><p>Relevance refers to how helpful or suitable a document is for a given query or task. For example, if you search for “My car broke down, what to do?” and there’s an article titled “Story: My car broke down, what I did,” the words may match closely, but that article isn’t really useful when you’re stranded on the road looking for guidance.</p><p>Most relevant result would be &ldquo;Automobile roadside assistance guide&rdquo;, no overlap with word but highly relevant. Relevance incorporates user-intent, context and importance of the terms. Relevance is context-dependent and asymmetric,</p><p>$$
relevance(Query, Doc) \neq relevance(Doc, Query)
$$</p><h3 id=similarity>Similarity<a hidden class=anchor aria-hidden=true href=#similarity>#</a></h3><p>Similarity is numrical measure of how terms are alike based on some metric and it is usally symmatric,</p><p>$$
similarity(A, B) == similarity(B, A)
$$</p><p>Similarity measures common features, words, structure or patterns and it is context-independent usually. It answers how these things look alike. A black shoe is highly similar with another black shoe of the same company.</p><h3 id=overlap>Overlap<a hidden class=anchor aria-hidden=true href=#overlap>#</a></h3><p>These concepts do overlap as,</p><blockquote><p>similarity is often a strong signal for relevance. In many cases, most relevant item is also the most similar one.</p></blockquote><p>As above, this relationship breaks down due to Synonymy & Language Variation (most relavant result may not be the most similar) and Polysemy & Ambiguity (High similar results can be irrelevant)</p><h3 id=what-did-we-calculate-above>What did we calculate above?<a hidden class=anchor aria-hidden=true href=#what-did-we-calculate-above>#</a></h3><p>Are TF-IDF and BM25 measures of relevance or similarity? Ranking functions primarily measure the similarity between a query and a document. This similarity serves as the basis for estimating relevance in search. In other words, ranking functions are similarity-based mechanisms designed to achieve relevance ranking.</p><h2 id=cosine-similarity>Cosine Similarity<a hidden class=anchor aria-hidden=true href=#cosine-similarity>#</a></h2><p>This metrics is used to measure how similar two vector are, irrespective of their length. There&rsquo;s awesome video on Youtube by Josh Starmer <a href="https://www.youtube.com/watch?v=e9U0QAFbfLI" target=_blank rel=noopener>Cosine Similarity, Clearly Explained!!!</a>
. Do watch!</p><p>Core idea is based on how to consider importance of the terms and overall direction of the content, not just the frequency of terms. It measures the cosine of the angle between two vectors.</p><ul><li>Small angle, both document point in same direction, High cosine similarity (close to 1), Very Similar</li><li>$90^\circ$ angle $cos(90^\circ) = 0$, No similarity</li><li>Obtuse angle, they point in opposite directions, negative cosine similarity, opposites</li></ul><p>Code snippet of measuring is Cosine Similarity</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.feature_extraction.text <span style=color:#f92672>import</span> TfidfVectorizer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics.pairwise <span style=color:#f92672>import</span> cosine_similarity
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Sample documents</span>
</span></span><span style=display:flex><span>docs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Looking for the item in a store?&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;On which aisle it is in a store?&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Looking for aisle 1A.&#34;</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ---------- Step 1: TF-IDF ----------</span>
</span></span><span style=display:flex><span>vectorizer <span style=color:#f92672>=</span> TfidfVectorizer()
</span></span><span style=display:flex><span>tfidf_matrix <span style=color:#f92672>=</span> vectorizer<span style=color:#f92672>.</span>fit_transform(docs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Convert to DataFrame for readability</span>
</span></span><span style=display:flex><span>tfidf_df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(
</span></span><span style=display:flex><span>    tfidf_matrix<span style=color:#f92672>.</span>toarray(),
</span></span><span style=display:flex><span>    columns<span style=color:#f92672>=</span>vectorizer<span style=color:#f92672>.</span>get_feature_names_out(),
</span></span><span style=display:flex><span>    index<span style=color:#f92672>=</span>[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Doc</span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(docs))]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;=== TF-IDF Matrix ===&#34;</span>)
</span></span><span style=display:flex><span>print(tfidf_df, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ---------- Step 2: Cosine Similarity ----------</span>
</span></span><span style=display:flex><span>cos_sim <span style=color:#f92672>=</span> cosine_similarity(tfidf_matrix)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cos_sim_df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(
</span></span><span style=display:flex><span>    cos_sim,
</span></span><span style=display:flex><span>    index<span style=color:#f92672>=</span>[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Doc</span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(docs))],
</span></span><span style=display:flex><span>    columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Doc</span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(docs))]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(cos_sim_df)
</span></span></code></pre></div><h2 id=bibliography>Bibliography<a hidden class=anchor aria-hidden=true href=#bibliography>#</a></h2><p>[1] Document-term matrix - <a href=https://en.wikipedia.org/wiki/Document-term_matrix target=_blank rel=noopener>https://en.wikipedia.org/wiki/Document-term_matrix</a>
[2] Okapi - BM25 - <a href=https://en.wikipedia.org/wiki/Okapi_BM25 target=_blank rel=noopener>https://en.wikipedia.org/wiki/Okapi_BM25</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.gandhijay.com/tags/ai/>Ai</a></li><li><a href=https://www.gandhijay.com/tags/rag/>Rag</a></li><li><a href=https://www.gandhijay.com/tags/graphrag/>Graphrag</a></li><li><a href=https://www.gandhijay.com/tags/llm/>Llm</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://www.gandhijay.com/>Jay Gandhi</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>