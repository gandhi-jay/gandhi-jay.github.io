<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes on Write Amplification versus Read Perspiration | Jay Gandhi</title>
<meta name=keywords content="write,read,write-amplification,read-perspiration"><meta name=description content="Notes on ACM artilcle 'Write Amplification versus Read Perspiration' by Pat Helland"><meta name=author content="Jay Gandhi"><link rel=canonical href=https://www.gandhijay.com/posts/write-amplification-versus-read-perspiration/><link href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer src=https://www.gandhijay.com/sharer.js></script><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://www.gandhijay.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.gandhijay.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.gandhijay.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.gandhijay.com/apple-touch-icon.png><link rel=mask-icon href=https://www.gandhijay.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><script defer type=text/javascript>(function(e,t){if(!t.__SV){var n,s,o,i;window.mixpanel=t,t._i=[],t.init=function(e,n,o){function c(e,t){var n=t.split(".");2==n.length&&(e=e[n[0]],t=n[1]),e[t]=function(){e.push([t].concat(Array.prototype.slice.call(arguments,0)))}}var r,a=t;"undefined"!=typeof o?a=t[o]=[]:o="mixpanel",a.people=a.people||[],a.toString=function(e){var t="mixpanel";return"mixpanel"!==o&&(t+="."+o),e||(t+=" (stub)"),t},a.people.toString=function(){return a.toString(1)+".people (stub)"},i="disable time_event track track_pageview track_links track_forms track_with_groups add_group set_group remove_group register register_once alias unregister identify name_tag set_config reset opt_in_tracking opt_out_tracking has_opted_in_tracking has_opted_out_tracking clear_opt_in_out_tracking start_batch_senders people.set people.set_once people.unset people.increment people.append people.union people.track_charge people.clear_charges people.delete_user people.remove".split(" ");for(s=0;s<i.length;s++)c(a,i[s]);r="set set_once union unset remove delete".split(" "),a.get_group=function(){function n(e){t[e]=function(){call2_args=arguments,call2=[e].concat(Array.prototype.slice.call(call2_args,0)),a.push([s,call2])}}for(var t={},s=["get_group"].concat(Array.prototype.slice.call(arguments,0)),e=0;e<r.length;e++)n(r[e]);return t},t._i.push([e,n,o])},t.__SV=1.2,n=e.createElement("script"),n.type="text/javascript",n.async=!0,n.src="undefined"!=typeof MIXPANEL_CUSTOM_LIB_URL?MIXPANEL_CUSTOM_LIB_URL:"file:"===e.location.protocol&&"//cdn.mxpnl.com/libs/mixpanel-2-latest.min.js".match(/^\/\//)?"https://cdn.mxpnl.com/libs/mixpanel-2-latest.min.js":"//cdn.mxpnl.com/libs/mixpanel-2-latest.min.js",o=e.getElementsByTagName("script")[0],o.parentNode.insertBefore(n,o)}})(document,window.mixpanel||[]),mixpanel.init("be2e8be5b550fc13316d2eb8514fa3e1"),mixpanel.track("On my website"),console.log("Asdasd")</script><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-173531504-2","auto"),ga("send","pageview"))</script><meta property="og:title" content="Notes on Write Amplification versus Read Perspiration"><meta property="og:description" content="Notes on ACM artilcle 'Write Amplification versus Read Perspiration' by Pat Helland"><meta property="og:type" content="article"><meta property="og:url" content="https://www.gandhijay.com/posts/write-amplification-versus-read-perspiration/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-08T02:39:21+05:30"><meta property="article:modified_time" content="2022-07-08T02:39:21+05:30"><meta property="og:site_name" content="Jay Gandhi"><meta name=twitter:card content="summary"><meta name=twitter:title content="Notes on Write Amplification versus Read Perspiration"><meta name=twitter:description content="Notes on ACM artilcle 'Write Amplification versus Read Perspiration' by Pat Helland"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.gandhijay.com/posts/"},{"@type":"ListItem","position":2,"name":"Notes on Write Amplification versus Read Perspiration","item":"https://www.gandhijay.com/posts/write-amplification-versus-read-perspiration/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes on Write Amplification versus Read Perspiration","name":"Notes on Write Amplification versus Read Perspiration","description":"Notes on ACM artilcle 'Write Amplification versus Read Perspiration' by Pat Helland","keywords":["write","read","write-amplification","read-perspiration"],"articleBody":"This was shared by one of the colleagues on Relog server on Discord. The discussion was about how log-structured based databases are not meant to have a 1:1 read per write ratio. There are various ways to tune the data structure to favor one over the other. You can read the original article from here .\nAbout Pat, He is one of the top publishers in different subject areas like distributed architecture, database transaction processing, etc. You can find the details of his latest research articles on his ACM profile page https://dl.acm.org/profile/81100628190 . Blockquotes in this post are the notes, I’ve picked from his article.\nI’ll be posting new articles once or twice a week. Subscribe for free to receive new post updates .\nIndexing within a database About RDBMS and Indexing,\nrelational database and how indexing can optimize access while being transparent to the application. Updating an index meant another two disk accesses since the indices of a B+ tree didn’t fit in memory\nThe question is how many indexes? every column? each possible pair of the columns?\nmore indexing we did, the faster the read queries would become. The more indexing we did, the more our ability to update became slower than molasses.\nThese are the common tradeoff. Reading fast frequently means writing slow\nRow-store versus Column-store It’s natural to associate high-performance updates with ‘row-store’. Another approach is to organize data by columns.\nColumn-store has more efficient access to data when querying a subset of columns by eliminating the need to read all columns of the row.\nColumnar databases are super fast for querying because many logical rows with the same value are physically close to each other. Updating the column store is not easy\nUpdates are kept separately in an integrated row-store.\nUsually, new updates are stored in a small integrated row store. Queries check the smaller row-store and combine the result of the faster column-store to give an accurate answer. Periodically these row-store updates get merged with column-store. These can be done in cascading fashion, like level-based compaction in LSM(log-structured merge) tree. These are explained below in leveling merges in Log-structured merge (LSM) tree Insertion in column-store(buffered into row-store really), you are incurring a debt to be paid later. This debt to rewrite and integrate the new data is a form of write amplification where a single write turns into more writes later.\nLSM: Log-structured Merge Trees LSM-tree was first proposed in 1996 by O’Neil et al. in this LSM-tree research article but there was no application over the decade. First, Google picked it up for their BigTable design. BigTable research-article idea is to track changes to a key-value store as transactions with new values are kept in the memory.\nIn key-value pair, the value can be multiple field values, JSON, blob, or anything. LSM tree has an in-memory buffer and leveled storage as shown in the image below.\nAs transaction commit, sorted collection of recent key-value pairs can be written to disk. This file contains the sorted key-value pairs along with an index to the keys in the file. Once written to disk, newly committed changes do not need to be kept in memory\nWhenever new data is inserted into the system, Log-structured merge (LSM) tree, data is going to be in the memory buffer. When the memory buffer is full (maximum size of the buffer reaches), the buffer will be sorted and flushed onto the disk as an immutable file (there will be no in-place updates). This results in compact storage and good ingestion throughput but results in read perspiration.\nTo reduce read perspiration, the Log-structured merge (LSM) tree invests energy to organize the data by rewriting it as you go. To make it easy to find keys, these are merged with files that were written earlier. Each log-structured merge (LSM) tree has some form of fan-out where lower levels of the tree are kept across more files.\nHere organizing data means once the buffer is flushed to an immutable file, compaction happens. Compaction is the process to merge two or more sorted immutable files. In the LSM tree, updates are out-of-place (This result in space amplification as duplicate values are getting stored). The update gets inserted as another key-value pair and the previously stored pair is logically invalidated. While merging two files, a previously written key-value pair that is now logically invalidated gets removed.\nLSM tree depends on the fan-out, the size of each file, and the number of key-value pairs in the tree. In general, most of the storage is in the lowest level of the tree.\nThe frequency of compaction and amount of data merged during compaction is the tunable parameter of the LSM tree to solve write amplification or read perspiration.\nLeveling merges When a new file is added to a level, pick the next file in the round-robin traversal and merge it with the files next level below.\nIf the next level down is grown above the nominal size, it will move one level down and merge with it. This process gets repeated if the level is grown above the nominal size in cascading fashion. By doing this periodically, there will be low space amplification, and reads will be faster\nLeveling merges have a large write-amplification. Each write of a new key-value pair to the first level will be written multiple times at each level it moves through. On other hand, they have small read perspiration, as the query typically checks only one place per level.\nTiering merges different but the related approach, files get to stack up on each level before doing the merge. This dramatically reduces the amount of merging required. Tiering merges have lower write amplification but larger read perspiration. Files are stacked up so less merging and hence less writing.\nReads need to be checked in lot more places leading to larger read perspiration\nIndexing and Searching Seach is in many ways a variety of database indexing. Search systems are a bit different in that they deal with the document. Most search systems asynchronously update the search index after the change to the document occurs.\nSearch makes reading the documents a lot easier. It dramatically lowers the read perspiration.\nUpdates to the documents asynchronously impose a debt onto the system to get them indexed. Creating and merging search indices is a complex job which is a form of *write amplification.\nTo index, you need to scour the corpus to find recently written or updated documents. Each of these needs to have an identifier and then needs to be processed to locate the search terms (n-grams)\nEach of these many n-grams found in a typical document then needs to be sent to an indexer that covers one of many shards. So, the document identifier now is associated with each term (or n-gram) located in the searchable document.\nAll of these because there was an update or creation of the document. write amplification\nInternet-scale search systems clearly offer excellent and low read perspiration.\nLarge-scale Caches Lots of big Internet systems have ginormous caches. Whenever anything changes, lots of servers are updated. This makes for a very easy and fast read in exchange for larger write amplification.\nThere will be a different article in the future for the large-scale caches. I’ll be covering research articles published by Meta (Facebook) and Twitter. I will be posting new articles on my website. Subscribe for free to receive new post updates .\nNormalization and Denormalization Working to avoid update anomalies was deemed to be extremely important. Performing a large number of joins to get an answer was a small penalty to pay to ensure the database wasn’t damaged by an errant update.\nMost systems are getting more and more distributed. Most of these have key-value pairs containing their data, which is sharded for scale.\nIf you were to normalize the data in this big and sharded system, the normalized values would not be on the same shard together. Doing a distributed join is more annoying than doing a centralized join.\nTo cope with this, people superimpose versioning on their data. It’s not perfect but it’s less challenging than distributed joins or trying to do massive updates across the denormalized data.\nLarge-scale distributed systems put a lot of pressure on the semantics of a consistent read. This, in turn, can be seen as a tension between write amplification and read perspiration.\nConclusion We’ve looked at just a few of the examples where there are tradeoffs in our systems between write and read. We see emerging systems that adapt and optimize for these tradeoffs as they watch their usage patterns.\nThis was an excellent read. You can go further through the points in depth to learn how distributed system tunes their data structure, indexing strategies, compaction, frequency of cache invalidation, etc based on their usage pattern.\nSubscribe for free to receive new post updates .\n","wordCount":"1482","inLanguage":"en","datePublished":"2022-07-08T02:39:21+05:30","dateModified":"2022-07-08T02:39:21+05:30","author":{"@type":"Person","name":"Jay Gandhi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.gandhijay.com/posts/write-amplification-versus-read-perspiration/"},"publisher":{"@type":"Organization","name":"Jay Gandhi","logo":{"@type":"ImageObject","url":"https://www.gandhijay.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.gandhijay.com/ accesskey=h title="Jay Gandhi (Alt + H)">Jay Gandhi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.gandhijay.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://www.gandhijay.com/about/ title=About><span>About</span></a></li><li><a href=https://www.gandhijay.com/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.gandhijay.com/>Home</a>&nbsp;»&nbsp;<a href=https://www.gandhijay.com/posts/>Posts</a></div><h1 class=post-title>Notes on Write Amplification versus Read Perspiration</h1><div class=post-description>Notes on ACM artilcle 'Write Amplification versus Read Perspiration' by Pat Helland</div><div class=post-meta>&lt;span title='2022-07-08 02:39:21 +0530 IST'>July 8, 2022&lt;/span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#indexing-within-a-database>Indexing within a database</a></li><li><a href=#row-store-versus-column-store>Row-store versus Column-store</a></li><li><a href=#lsm-log-structured-merge-trees>LSM: Log-structured Merge Trees</a><ul><li><a href=#leveling-merges>Leveling merges</a></li><li><a href=#tiering-merges>Tiering merges</a></li></ul></li><li><a href=#indexing-and-searching>Indexing and Searching</a></li><li><a href=#large-scale-caches>Large-scale Caches</a></li><li><a href=#normalization-and-denormalization>Normalization and Denormalization</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><p>This was shared by one of the colleagues on Relog server on Discord. The discussion was about how log-structured based databases are not meant to have a 1:1 read per write ratio. There are various ways to tune the data structure to favor one over the other. You can read the original article from <a href=https://link.gandhijay.com/write-amplification-versus-read-perspiration target=_blank rel=noopener>here </a>.</p><p>About Pat, He is one of the top publishers in different subject areas like distributed architecture, database transaction processing, etc. You can find the details of his latest research articles on his ACM profile page <a href=https://dl.acm.org/profile/81100628190 target=_blank rel=noopener>https://dl.acm.org/profile/81100628190 </a>. Blockquotes in this post are the notes, I&rsquo;ve picked from his article.</p><p>I&rsquo;ll be posting new articles once or twice a week. <a href=https://link.gandhijay.com/substack-subscribe target=_blank rel=noopener>Subscribe for free to receive new post updates </a>.</p><h2 id=indexing-within-a-database>Indexing within a database<a hidden class=anchor aria-hidden=true href=#indexing-within-a-database>#</a></h2><p>About RDBMS and Indexing,</p><blockquote><p>relational database and how indexing can optimize access while being transparent to the application. Updating an index meant another two disk accesses since the indices of a B+ tree didn&rsquo;t fit in memory</p></blockquote><p>The question is how many indexes? every column? each possible pair of the columns?</p><blockquote><p>more indexing we did, the faster the read queries would become. The more indexing we did, the more our ability to update became slower than molasses.</p></blockquote><p>These are the common tradeoff. <em><strong>Reading fast frequently means writing slow</strong></em></p><h2 id=row-store-versus-column-store>Row-store versus Column-store<a hidden class=anchor aria-hidden=true href=#row-store-versus-column-store>#</a></h2><blockquote><p>It&rsquo;s natural to associate high-performance updates with &lsquo;row-store&rsquo;. Another approach is to organize data by columns.</p></blockquote><p>Column-store has more efficient access to data when querying a subset of columns by eliminating the need to read all columns of the row.</p><blockquote><p>Columnar databases are super fast for querying because many logical rows with the same value are physically close to each other. <strong>Updating the column store is not easy</strong></p></blockquote><blockquote><p>Updates are kept separately in an integrated row-store.</p></blockquote><p>Usually, new updates are stored in a small integrated row store. Queries check the smaller row-store and combine the result of the faster column-store to give an accurate answer. Periodically these row-store updates get merged with column-store. These can be done in cascading fashion, like level-based compaction in LSM(log-structured merge) tree. These are explained below in <a href=#leveling-merges>leveling merges in Log-structured merge (LSM) tree</a></p><blockquote><p>Insertion in column-store(buffered into row-store really), you are incurring a debt to be paid later. This debt to rewrite and integrate the new data is a form of <em><strong>write amplification</strong></em> where a single write turns into more writes later.</p></blockquote><h2 id=lsm-log-structured-merge-trees>LSM: Log-structured Merge Trees<a hidden class=anchor aria-hidden=true href=#lsm-log-structured-merge-trees>#</a></h2><p>LSM-tree was first proposed in 1996 by O’Neil et al. in this <a href=https://link.gandhijay.com/the-log-structured-merge-lsm-tree target=_blank rel=noopener>LSM-tree research article </a>but there was no application over the decade. First, Google picked it up for their BigTable design.<a href=https://link.gandhijay.com/bigtable-dist-storage-system-for-structured-data target=_blank rel=noopener> BigTable research-article</a></p><blockquote><p>idea is to track changes to a key-value store as transactions with new values are kept in the memory.</p></blockquote><p>In key-value pair, the value can be multiple field values, JSON, blob, or anything. LSM tree has an in-memory buffer and leveled storage as shown in the image below.</p><p><img loading=lazy src=/research-paper/lsm-tree.jpg alt="Log-structured merge (LSM) tree"></p><blockquote><p>As transaction commit, sorted collection of recent key-value pairs can be written to disk. This file contains the sorted key-value pairs along with an index to the keys in the file. Once written to disk, newly committed changes do not need to be kept in memory</p></blockquote><p>Whenever new data is inserted into the system, Log-structured merge (LSM) tree, data is going to be in the memory buffer. When the memory buffer is full (maximum size of the buffer reaches), the buffer will be sorted and flushed onto the disk as an immutable file (there will be no in-place updates). This results in compact storage and good ingestion throughput but results in <em><strong>read perspiration</strong></em>.</p><blockquote><p>To reduce read perspiration, the Log-structured merge (LSM) tree invests energy to organize the data by rewriting it as you go. To make it easy to find keys, these are merged with files that were written earlier. Each log-structured merge (LSM) tree has some form of fan-out where lower levels of the tree are kept across more files.</p></blockquote><p>Here organizing data means once the buffer is flushed to an immutable file, compaction happens. Compaction is the process to merge two or more sorted immutable files. In the LSM tree, updates are out-of-place (This result in space amplification as duplicate values are getting stored). The update gets inserted as another key-value pair and the previously stored pair is logically invalidated. While merging two files, a previously written key-value pair that is now logically invalidated gets removed.</p><blockquote><p>LSM tree depends on the fan-out, the size of each file, and the number of key-value pairs in the tree. In general, <em><strong>most of the storage is in the lowest level of the tree</strong></em>.</p></blockquote><p>The frequency of compaction and amount of data merged during compaction is the tunable parameter of the LSM tree to solve write amplification or read perspiration.</p><h3 id=leveling-merges>Leveling merges<a hidden class=anchor aria-hidden=true href=#leveling-merges>#</a></h3><blockquote><p>When a new file is added to a level, pick the next file in the round-robin traversal and merge it with the files next level below.</p></blockquote><p>If the next level down is grown above the nominal size, it will move one level down and merge with it. This process gets repeated if the level is grown above the nominal size in cascading fashion. By doing this periodically, there will be low space amplification, and reads will be faster</p><blockquote><p>Leveling merges have a large <em><strong>write-amplification</strong></em>. Each write of a new key-value pair to the first level will be written multiple times at each level it moves through. On other hand, they have small read perspiration, as the query typically checks only one place per level.</p></blockquote><h3 id=tiering-merges>Tiering merges<a hidden class=anchor aria-hidden=true href=#tiering-merges>#</a></h3><blockquote><p>different but the related approach, files get to stack up on each level before doing the merge. This dramatically reduces the amount of merging required. Tiering merges have lower write amplification but larger read perspiration. Files are stacked up so less merging and hence less writing.</p></blockquote><blockquote><p>Reads need to be checked in lot more places leading to larger <em><strong>read perspiration</strong></em></p></blockquote><h2 id=indexing-and-searching>Indexing and Searching<a hidden class=anchor aria-hidden=true href=#indexing-and-searching>#</a></h2><blockquote><p>Seach is in many ways a variety of database indexing. Search systems are a bit different in that they deal with the document. Most search systems asynchronously update the search index after the change to the document occurs.</p></blockquote><blockquote><p>Search makes reading the documents a lot easier. It dramatically lowers the <em><strong>read perspiration</strong></em>.</p></blockquote><blockquote><p>Updates to the documents asynchronously impose a debt onto the system to get them indexed. Creating and merging search indices is a complex job which is a form of *<strong>write amplification</strong>.</p></blockquote><blockquote><p>To index, you need to scour the corpus to find recently written or updated documents. Each of these needs to have an identifier and then needs to be processed to locate the search terms (n-grams)</p></blockquote><blockquote><p>Each of these many n-grams found in a typical document then needs to be sent to an indexer that covers one of many shards. So, the document identifier now is associated with each term (or n-gram) located in the searchable document.</p></blockquote><blockquote><p>All of these because there was an update or creation of the document. <em><strong>write amplification</strong></em></p></blockquote><blockquote><p>Internet-scale search systems clearly offer excellent and low read perspiration.</p></blockquote><h2 id=large-scale-caches>Large-scale Caches<a hidden class=anchor aria-hidden=true href=#large-scale-caches>#</a></h2><blockquote><p>Lots of big Internet systems have ginormous caches. Whenever anything changes, lots of servers are updated. This makes for a very easy and fast read in exchange for larger <em><strong>write amplification</strong></em>.</p></blockquote><p>There will be a different article in the future for the large-scale caches. I&rsquo;ll be covering research articles published by Meta (Facebook) and Twitter. I will be posting new articles on my website. <a href=https://link.gandhijay.com/substack-subscribe target=_blank rel=noopener>Subscribe for free to receive new post updates </a>.</p><h2 id=normalization-and-denormalization>Normalization and Denormalization<a hidden class=anchor aria-hidden=true href=#normalization-and-denormalization>#</a></h2><blockquote><p>Working to avoid update anomalies was deemed to be extremely important. Performing a large number of joins to get an answer was a small penalty to pay to ensure the database wasn’t damaged by an errant update.</p></blockquote><blockquote><p>Most systems are getting more and more distributed. Most of these have key-value pairs containing their data, which is sharded for scale.</p></blockquote><blockquote><p>If you were to normalize the data in this big and sharded system, the normalized values would not be on the same shard together. Doing a distributed join is more annoying than doing a centralized join.</p></blockquote><blockquote><p>To cope with this, people superimpose versioning on their data. It’s not perfect but it’s less challenging than distributed joins or trying to do massive updates across the denormalized data.</p></blockquote><blockquote><p>Large-scale distributed systems put a lot of pressure on the semantics of a consistent read. This, in turn, can be seen as a tension between <em><strong>write amplification</strong></em> and <em><strong>read perspiration</strong></em>.</p></blockquote><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><blockquote><p>We’ve looked at just a few of the examples where there are tradeoffs in our systems between write and read. We see emerging systems that adapt and optimize for these tradeoffs as they watch their usage patterns.</p></blockquote><p>This was an excellent read. You can go further through the points in depth to learn how distributed system tunes their data structure, indexing strategies, compaction, frequency of cache invalidation, etc based on their usage pattern.</p><p><a href=https://link.gandhijay.com/substack-subscribe target=_blank rel=noopener>Subscribe for free to receive new post updates </a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.gandhijay.com/tags/write/>write</a></li><li><a href=https://www.gandhijay.com/tags/read/>read</a></li><li><a href=https://www.gandhijay.com/tags/write-amplification/>write-amplification</a></li><li><a href=https://www.gandhijay.com/tags/read-perspiration/>read-perspiration</a></li></ul><div class=share-buttons><a href=javascript:void(0); title=" Twitter" data-sharer=twitter data-url=https://www.gandhijay.com/posts/write-amplification-versus-read-perspiration/ data-title="Notes on Write Amplification versus Read Perspiration" data-hashtags=write,read,write-amplification,read-perspiration><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a href=javascript:void(0); title=" Linkedin" data-sharer=linkedin data-url=https://www.gandhijay.com/posts/write-amplification-versus-read-perspiration/><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://www.gandhijay.com/>Jay Gandhi</a></span>
<span>Powered by Hugo and PaperMod</span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>