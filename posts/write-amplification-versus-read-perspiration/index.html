<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes on Write Amplification versus Read Perspiration | Jay Gandhi</title><meta name=keywords content="write,read,write-amplification,read-perspiration"><meta name=description content="Notes on ACM artilcle 'Write Amplification versus Read Perspiration' by Pat Helland"><meta name=author content="Jay Gandhi"><link rel=canonical href=https://www.gandhijay.com/posts/write-amplification-versus-read-perspiration/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://www.gandhijay.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.gandhijay.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.gandhijay.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.gandhijay.com/apple-touch-icon.png><link rel=mask-icon href=https://www.gandhijay.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://www.gandhijay.com/posts/write-amplification-versus-read-perspiration/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-R83NFY43QT"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R83NFY43QT")}</script><meta property="og:url" content="https://www.gandhijay.com/posts/write-amplification-versus-read-perspiration/"><meta property="og:site_name" content="Jay Gandhi"><meta property="og:title" content="Notes on Write Amplification versus Read Perspiration"><meta property="og:description" content="Notes on ACM artilcle 'Write Amplification versus Read Perspiration' by Pat Helland"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-08T02:39:21+05:30"><meta property="article:modified_time" content="2022-07-08T02:39:21+05:30"><meta property="article:tag" content="Write"><meta property="article:tag" content="Read"><meta property="article:tag" content="Write-Amplification"><meta property="article:tag" content="Read-Perspiration"><meta name=twitter:card content="summary"><meta name=twitter:title content="Notes on Write Amplification versus Read Perspiration"><meta name=twitter:description content="Notes on ACM artilcle 'Write Amplification versus Read Perspiration' by Pat Helland"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.gandhijay.com/posts/"},{"@type":"ListItem","position":2,"name":"Notes on Write Amplification versus Read Perspiration","item":"https://www.gandhijay.com/posts/write-amplification-versus-read-perspiration/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes on Write Amplification versus Read Perspiration","name":"Notes on Write Amplification versus Read Perspiration","description":"Notes on ACM artilcle 'Write Amplification versus Read Perspiration' by Pat Helland","keywords":["write","read","write-amplification","read-perspiration"],"articleBody":"This was shared by a colleague. The discussion centered on how log-structured databases are not designed for a 1:1 read-to-write ratio. There are various ways to tune the data structure to favor one over the other. You can read the original article here .\nAbout Pat: He is a leading author in areas like distributed architecture, database transaction processing, and more. You can find his latest research on his ACM profile page . Blockquotes in this post are excerpts from his article.\nI’ll be posting new articles once or twice a week. Subscribe for free to receive updates .\nIndexing within a database Regarding RDBMS and indexing:\nrelational database and how indexing can optimize access while being transparent to the application. Updating an index meant another two disk accesses since the indices of a B+ tree didn’t fit in memory\nThe question is: how many indexes? One per column? Every possible pair of columns?\nmore indexing we did, the faster the read queries would become. The more indexing we did, the more our ability to update became slower than molasses.\nThese are common tradeoffs. Reading fast frequently means writing slow.\nRow-store versus Column-store It’s natural to associate high-performance updates with ‘row-store’. Another approach is to organize data by columns.\nColumn-stores allow more efficient access to data when querying a subset of columns, as they avoid reading unnecessary columns.\nColumnar databases are super fast for querying because many logical rows with the same value are physically close to each other. Updating the column store is not easy\nUpdates are kept separately in an integrated row-store.\nTypically, updates go to a small row-store, and queries combine results from this and the column-store. Periodically, these updates are merged into the column-store, often using a cascading process like level-based compaction in LSM trees (explained in the next section ).\nInsertion in column-store(buffered into row-store really), you are incurring a debt to be paid later. This debt to rewrite and integrate the new data is a form of write amplification where a single write turns into more writes later.\nLSM: Log-structured Merge Trees LSM trees were first proposed in 1996 by O’Neil et al. in this research article , but lacked adoption for years—until Google used them in BigTable .\nidea is to track changes to a key-value store as transactions with new values are kept in the memory.\nIn key-value pair, the value can be multiple field values, JSON, blob, or anything. LSM tree has an in-memory buffer and leveled storage as shown in the image below. As transaction commit, sorted collection of recent key-value pairs can be written to disk. This file contains the sorted key-value pairs along with an index to the keys in the file. Once written to disk, newly committed changes do not need to be kept in memory\nIn LSM trees, new data is inserted into an in-memory buffer. When full, this buffer is sorted and flushed to disk as an immutable file—resulting in compact storage and high ingestion throughput, but also read perspiration.\nTo reduce read perspiration, the Log-structured merge (LSM) tree invests energy to organize the data by rewriting it as you go. To make it easy to find keys, these are merged with files that were written earlier. Each log-structured merge (LSM) tree has some form of fan-out where lower levels of the tree are kept across more files.\nHere organizing data means once the buffer is flushed to an immutable file, compaction happens. Compaction is the process to merge two or more sorted immutable files. In the LSM tree, updates are out-of-place (This result in space amplification as duplicate values are getting stored). The update gets inserted as another key-value pair and the previously stored pair is logically invalidated. While merging two files, a previously written key-value pair that is now logically invalidated gets removed.\nLSM tree depends on the fan-out, the size of each file, and the number of key-value pairs in the tree. In general, most of the storage is in the lowest level of the tree.\nThe frequency of compaction and amount of data merged during compaction is the tunable parameter of the LSM tree to solve write amplification or read perspiration.\nLeveling merges When a new file is added to a level, pick the next file in the round-robin traversal and merge it with the files next level below.\nIf the next level down is grown above the nominal size, it will move one level down and merge with it. This process gets repeated if the level is grown above the nominal size in cascading fashion. By doing this periodically, there will be low space amplification, and reads will be faster\nLeveling merges have a large write-amplification. Each write of a new key-value pair to the first level will be written multiple times at each level it moves through. On other hand, they have small read perspiration, as the query typically checks only one place per level.\nTiering merges different but the related approach, files get to stack up on each level before doing the merge. This dramatically reduces the amount of merging required. Tiering merges have lower write amplification but larger read perspiration. Files are stacked up so less merging and hence less writing.\nReads need to be checked in lot more places leading to larger read perspiration\nIndexing and Searching Seach is in many ways a variety of database indexing. Search systems are a bit different in that they deal with the document. Most search systems asynchronously update the search index after the change to the document occurs.\nSearch makes reading the documents a lot easier. It dramatically lowers the read perspiration.\nUpdates to the documents asynchronously impose a debt onto the system to get them indexed. Creating and merging search indices is a complex job which is a form of *write amplification.\nTo index, you need to scour the corpus to find recently written or updated documents. Each of these needs to have an identifier and then needs to be processed to locate the search terms (n-grams)\nEach of these many n-grams found in a typical document then needs to be sent to an indexer that covers one of many shards. So, the document identifier now is associated with each term (or n-gram) located in the searchable document.\nAll of these because there was an update or creation of the document. write amplification\nInternet-scale search systems clearly offer excellent and low read perspiration.\nLarge-scale Caches Lots of big Internet systems have ginormous caches. Whenever anything changes, lots of servers are updated. This makes for a very easy and fast read in exchange for larger write amplification.\nThere will be a different article in the future for the large-scale caches. I’ll be covering research articles published by Meta (Facebook) and Twitter. I will be posting new articles on my website. Subscribe for free to receive new post updates .\nNormalization and Denormalization Working to avoid update anomalies was deemed to be extremely important. Performing a large number of joins to get an answer was a small penalty to pay to ensure the database wasn’t damaged by an errant update.\nMost systems are getting more and more distributed. Most of these have key-value pairs containing their data, which is sharded for scale.\nIf you were to normalize the data in this big and sharded system, the normalized values would not be on the same shard together. Doing a distributed join is more annoying than doing a centralized join.\nTo cope with this, people superimpose versioning on their data. It’s not perfect but it’s less challenging than distributed joins or trying to do massive updates across the denormalized data.\nLarge-scale distributed systems put a lot of pressure on the semantics of a consistent read. This, in turn, can be seen as a tension between write amplification and read perspiration.\nConclusion We’ve looked at just a few of the examples where there are tradeoffs in our systems between write and read. We see emerging systems that adapt and optimize for these tradeoffs as they watch their usage patterns.\nThis article provides a snapshot of how distributed systems balance data structure design, indexing, compaction, and cache invalidation in the context of real-world workloads. Subscribe for free to receive new post updates .\n","wordCount":"1379","inLanguage":"en","datePublished":"2022-07-08T02:39:21+05:30","dateModified":"2022-07-08T02:39:21+05:30","author":{"@type":"Person","name":"Jay Gandhi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.gandhijay.com/posts/write-amplification-versus-read-perspiration/"},"publisher":{"@type":"Organization","name":"Jay Gandhi","logo":{"@type":"ImageObject","url":"https://www.gandhijay.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.gandhijay.com/ accesskey=h title="Jay Gandhi (Alt + H)">Jay Gandhi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.gandhijay.com/about/ title=About><span>About</span></a></li><li><a href=https://www.gandhijay.com/ai/ title="AI Days"><span>AI Days</span></a></li><li><a href=https://www.gandhijay.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://www.gandhijay.com/bookshelf/ title=Bookshelf><span>Bookshelf</span></a></li><li><a href=https://www.gandhijay.com/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.gandhijay.com/>Home</a>&nbsp;»&nbsp;<a href=https://www.gandhijay.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Notes on Write Amplification versus Read Perspiration</h1><div class=post-description>Notes on ACM artilcle 'Write Amplification versus Read Perspiration' by Pat Helland</div><div class=post-meta><span title='2022-07-08 02:39:21 +0530 IST'>July 8, 2022</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Jay Gandhi</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#indexing-within-a-database>Indexing within a database</a></li><li><a href=#row-store-versus-column-store>Row-store versus Column-store</a></li><li><a href=#lsm-log-structured-merge-trees>LSM: Log-structured Merge Trees</a><ul><li><a href=#leveling-merges>Leveling merges</a></li><li><a href=#tiering-merges>Tiering merges</a></li></ul></li><li><a href=#indexing-and-searching>Indexing and Searching</a></li><li><a href=#large-scale-caches>Large-scale Caches</a></li><li><a href=#normalization-and-denormalization>Normalization and Denormalization</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><p>This was shared by a colleague. The discussion centered on how log-structured databases are not designed for a 1:1 read-to-write ratio. There are various ways to tune the data structure to favor one over the other. You can read the original article <a href=https://link.gandhijay.com/write-amplification-versus-read-perspiration target=_blank rel=noopener>here</a>
.</p><p>About Pat: He is a leading author in areas like distributed architecture, database transaction processing, and more. You can find his latest research on his <a href=https://dl.acm.org/profile/81100628190 target=_blank rel=noopener>ACM profile page</a>
. Blockquotes in this post are excerpts from his article.</p><p>I&rsquo;ll be posting new articles once or twice a week. <a href=https://link.gandhijay.com/substack-subscribe target=_blank rel=noopener>Subscribe for free to receive updates</a>
.</p><h2 id=indexing-within-a-database>Indexing within a database<a hidden class=anchor aria-hidden=true href=#indexing-within-a-database>#</a></h2><p>Regarding RDBMS and indexing:</p><blockquote><p>relational database and how indexing can optimize access while being transparent to the application. Updating an index meant another two disk accesses since the indices of a B+ tree didn&rsquo;t fit in memory</p></blockquote><p>The question is: how many indexes? One per column? Every possible pair of columns?</p><blockquote><p>more indexing we did, the faster the read queries would become. The more indexing we did, the more our ability to update became slower than molasses.</p></blockquote><p>These are common tradeoffs. <em><strong>Reading fast frequently means writing slow.</strong></em></p><h2 id=row-store-versus-column-store>Row-store versus Column-store<a hidden class=anchor aria-hidden=true href=#row-store-versus-column-store>#</a></h2><blockquote><p>It&rsquo;s natural to associate high-performance updates with &lsquo;row-store&rsquo;. Another approach is to organize data by columns.</p></blockquote><p>Column-stores allow more efficient access to data when querying a subset of columns, as they avoid reading unnecessary columns.</p><blockquote><p>Columnar databases are super fast for querying because many logical rows with the same value are physically close to each other. <strong>Updating the column store is not easy</strong></p></blockquote><blockquote><p>Updates are kept separately in an integrated row-store.</p></blockquote><p>Typically, updates go to a small row-store, and queries combine results from this and the column-store. Periodically, these updates are merged into the column-store, often using a cascading process like level-based compaction in LSM trees (explained in the <a href=#leveling-merges>next section</a>
).</p><blockquote><p>Insertion in column-store(buffered into row-store really), you are incurring a debt to be paid later. This debt to rewrite and integrate the new data is a form of <em><strong>write amplification</strong></em> where a single write turns into more writes later.</p></blockquote><h2 id=lsm-log-structured-merge-trees>LSM: Log-structured Merge Trees<a hidden class=anchor aria-hidden=true href=#lsm-log-structured-merge-trees>#</a></h2><p>LSM trees were first proposed in 1996 by O’Neil et al. in this <a href=https://link.gandhijay.com/the-log-structured-merge-lsm-tree target=_blank rel=noopener>research article</a>
, but lacked adoption for years—until Google used them in <a href=https://link.gandhijay.com/bigtable-dist-storage-system-for-structured-data target=_blank rel=noopener>BigTable</a>
.</p><blockquote><p>idea is to track changes to a key-value store as transactions with new values are kept in the memory.</p></blockquote><p>In key-value pair, the value can be multiple field values, JSON, blob, or anything. LSM tree has an in-memory buffer and leveled storage as shown in the image below.
<img alt="Log-structured merge (LSM) tree" loading=lazy src=/research-paper/lsm-tree.jpg></p><blockquote><p>As transaction commit, sorted collection of recent key-value pairs can be written to disk. This file contains the sorted key-value pairs along with an index to the keys in the file. Once written to disk, newly committed changes do not need to be kept in memory</p></blockquote><p>In LSM trees, new data is inserted into an in-memory buffer. When full, this buffer is sorted and flushed to disk as an immutable file—resulting in compact storage and high ingestion throughput, but also <em><strong>read perspiration</strong></em>.</p><blockquote><p>To reduce read perspiration, the Log-structured merge (LSM) tree invests energy to organize the data by rewriting it as you go. To make it easy to find keys, these are merged with files that were written earlier. Each log-structured merge (LSM) tree has some form of fan-out where lower levels of the tree are kept across more files.</p></blockquote><p>Here organizing data means once the buffer is flushed to an immutable file, compaction happens. Compaction is the process to merge two or more sorted immutable files. In the LSM tree, updates are out-of-place (This result in space amplification as duplicate values are getting stored). The update gets inserted as another key-value pair and the previously stored pair is logically invalidated. While merging two files, a previously written key-value pair that is now logically invalidated gets removed.</p><blockquote><p>LSM tree depends on the fan-out, the size of each file, and the number of key-value pairs in the tree. In general, <em><strong>most of the storage is in the lowest level of the tree</strong></em>.</p></blockquote><p>The frequency of compaction and amount of data merged during compaction is the tunable parameter of the LSM tree to solve write amplification or read perspiration.</p><h3 id=leveling-merges>Leveling merges<a hidden class=anchor aria-hidden=true href=#leveling-merges>#</a></h3><blockquote><p>When a new file is added to a level, pick the next file in the round-robin traversal and merge it with the files next level below.</p></blockquote><p>If the next level down is grown above the nominal size, it will move one level down and merge with it. This process gets repeated if the level is grown above the nominal size in cascading fashion. By doing this periodically, there will be low space amplification, and reads will be faster</p><blockquote><p>Leveling merges have a large <em><strong>write-amplification</strong></em>. Each write of a new key-value pair to the first level will be written multiple times at each level it moves through. On other hand, they have small read perspiration, as the query typically checks only one place per level.</p></blockquote><h3 id=tiering-merges>Tiering merges<a hidden class=anchor aria-hidden=true href=#tiering-merges>#</a></h3><blockquote><p>different but the related approach, files get to stack up on each level before doing the merge. This dramatically reduces the amount of merging required. Tiering merges have lower write amplification but larger read perspiration. Files are stacked up so less merging and hence less writing.</p></blockquote><blockquote><p>Reads need to be checked in lot more places leading to larger <em><strong>read perspiration</strong></em></p></blockquote><h2 id=indexing-and-searching>Indexing and Searching<a hidden class=anchor aria-hidden=true href=#indexing-and-searching>#</a></h2><blockquote><p>Seach is in many ways a variety of database indexing. Search systems are a bit different in that they deal with the document. Most search systems asynchronously update the search index after the change to the document occurs.</p></blockquote><blockquote><p>Search makes reading the documents a lot easier. It dramatically lowers the <em><strong>read perspiration</strong></em>.</p></blockquote><blockquote><p>Updates to the documents asynchronously impose a debt onto the system to get them indexed. Creating and merging search indices is a complex job which is a form of *<strong>write amplification</strong>.</p></blockquote><blockquote><p>To index, you need to scour the corpus to find recently written or updated documents. Each of these needs to have an identifier and then needs to be processed to locate the search terms (n-grams)</p></blockquote><blockquote><p>Each of these many n-grams found in a typical document then needs to be sent to an indexer that covers one of many shards. So, the document identifier now is associated with each term (or n-gram) located in the searchable document.</p></blockquote><blockquote><p>All of these because there was an update or creation of the document. <em><strong>write amplification</strong></em></p></blockquote><blockquote><p>Internet-scale search systems clearly offer excellent and low read perspiration.</p></blockquote><h2 id=large-scale-caches>Large-scale Caches<a hidden class=anchor aria-hidden=true href=#large-scale-caches>#</a></h2><blockquote><p>Lots of big Internet systems have ginormous caches. Whenever anything changes, lots of servers are updated. This makes for a very easy and fast read in exchange for larger <em><strong>write amplification</strong></em>.</p></blockquote><p>There will be a different article in the future for the large-scale caches. I&rsquo;ll be covering research articles published by Meta (Facebook) and Twitter. I will be posting new articles on my website. <a href=https://link.gandhijay.com/substack-subscribe target=_blank rel=noopener>Subscribe for free to receive new post updates</a>
.</p><h2 id=normalization-and-denormalization>Normalization and Denormalization<a hidden class=anchor aria-hidden=true href=#normalization-and-denormalization>#</a></h2><blockquote><p>Working to avoid update anomalies was deemed to be extremely important. Performing a large number of joins to get an answer was a small penalty to pay to ensure the database wasn’t damaged by an errant update.</p></blockquote><blockquote><p>Most systems are getting more and more distributed. Most of these have key-value pairs containing their data, which is sharded for scale.</p></blockquote><blockquote><p>If you were to normalize the data in this big and sharded system, the normalized values would not be on the same shard together. Doing a distributed join is more annoying than doing a centralized join.</p></blockquote><blockquote><p>To cope with this, people superimpose versioning on their data. It’s not perfect but it’s less challenging than distributed joins or trying to do massive updates across the denormalized data.</p></blockquote><blockquote><p>Large-scale distributed systems put a lot of pressure on the semantics of a consistent read. This, in turn, can be seen as a tension between <em><strong>write amplification</strong></em> and <em><strong>read perspiration</strong></em>.</p></blockquote><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><blockquote><p>We’ve looked at just a few of the examples where there are tradeoffs in our systems between write and read. We see emerging systems that adapt and optimize for these tradeoffs as they watch their usage patterns.</p></blockquote><p>This article provides a snapshot of how distributed systems balance data structure design, indexing, compaction, and cache invalidation in the context of real-world workloads.
<a href=https://link.gandhijay.com/substack-subscribe target=_blank rel=noopener>Subscribe for free to receive new post updates</a>
.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.gandhijay.com/tags/write/>Write</a></li><li><a href=https://www.gandhijay.com/tags/read/>Read</a></li><li><a href=https://www.gandhijay.com/tags/write-amplification/>Write-Amplification</a></li><li><a href=https://www.gandhijay.com/tags/read-perspiration/>Read-Perspiration</a></li></ul><nav class=paginav><a class=prev href=https://www.gandhijay.com/posts/control-flow/><span class=title>« Prev</span><br><span>Flow of Control (Control Flow)</span>
</a><a class=next href=https://www.gandhijay.com/posts/apache-kafka-series-part-one/><span class=title>Next »</span><br><span>Logs [Apache Kafka 1 of N]</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://www.gandhijay.com/>Jay Gandhi</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>